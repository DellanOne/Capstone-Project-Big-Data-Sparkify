# Udacity Nanodegree big data Capstone Project--Sparkify


## Project Backgound
### Motivations
developing Skills of:  
1. Loading large datasets into Spark and manipulating them using Spark SQL and Spark Dataframes
2. Using the machine learning APIs within Spark ML to build and tune models
3. Integrating the skills I've learned in the Spark course and the Data Scientist Nanodegree program

### Task and Datasets 
the goad is to predict churned users based on activites and attributes data of them. And deploy the solution on a distributed system.
The size of original datasets is 12GB. Due to the limited computation power of free version of IBM Cloud, a medium-sized sub-datasets is utilized.  

### frameworks needed to run this jupyter notebook:
1. Pyspark SQL and Pyspark ML and other libraries it is building on.
2. Matplotlib for visualization.
3. IBM Cloud(free) or other Clould sevices.

## Other deliverables
Summary and some flections on this project: [Medium post](https://medium.com/@jlm3448179892009/get-my-hands-dirty-with-big-data-for-the-first-time-50788c975cef)