# Udacity Nanodegree big data Capstone Project--Sparkify


## Project Backgound
### Motivations
Skills required:  
1. Load large datasets into Spark and manipulate them using Spark SQL and Spark Dataframes
2. Use the machine learning APIs within Spark ML to build and tune models
3. Integrate the skills I've learned in the Spark course and the Data Scientist Nanodegree program

### Datasets and Task
the goad is to predict churned users based on activites and attributes data of them. And deploy the solution on a distributed system.
The size of original datasets is 12GB. Due to the limited computation power of free version of IBM Cloud, a medium-sized sub-datasets is utilized.  

### frameworks needed to run this jupyter notebook:
1. Pyspark SQL and Pyspark ML and other libraries it is building on.
2. IBM Cloud(free) or other Clould sevices.

## Other deliverables
Summary and some flections on this project: [Medium post](https://medium.com/@jlm3448179892009/get-my-hands-dirty-with-big-data-for-the-first-time-50788c975cef)